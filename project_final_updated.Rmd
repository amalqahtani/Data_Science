---
title: "Covid19 Project"
author: "Team A"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:  
    rmdformats::readthedown:
      toc_float: true
      number_sections: true
      code_folding: hide
      keep_tex: yes
---



```{r setup, echo=FALSE, cache=FALSE}
  library(knitr)
library(rmdformats)
## Global options
options(max.print="75", scipen = 999, digits = 3, big.mark=",", warn = -1)
opts_chunk$set(echo=FALSE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

```{r basicfunct, include=FALSE}
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```

```{r libraries}
#library(ggplot2)
#require(gridExtra)
#library(kableExtra)
loadPkg("ggplot2")
loadPkg("gridExtra")
loadPkg("kableExtra")
loadPkg("data.table") 
```

```{r xkablesummary}
loadPkg("xtable")
loadPkg("kableExtra")
loadPkg("stringi")

xkabledply = function(modelsmmrytable, title="Table", digits = 4, pos="left", bso="striped", wide=FALSE) { 
  #' Combining base::summary, xtable, and kableExtra, to easily display model summary. 
  #' wrapper for the base::summary function on model objects
  #' Can also use as head for better display
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param modelsmmrytable This can be a generic table, a model object such as lm(), or the summary of a model object summary(lm()) 
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @param wide print table in long (FALSE) format or wide (TRUE) format
  #' @return HTML table for display
  #' @examples
  #' library("xtable")
  #' library("kableExtra")
  #' xkabledply( df, title="Table testing", pos="left", bso="hover" )
  #' xkabledply( ISLR::Hitters[1:5,] )
  if (wide) { modelsmmrytable <- t(modelsmmrytable) }
  modelsmmrytable %>%
    xtable() %>% 
    kable(caption = title, digits = digits) %>%
    kable_styling(bootstrap_options = bso, full_width = FALSE, position = pos)
}

xkabledplyhead = function(df, rows=5, title="Head", digits = 4, pos="left", bso="striped") { 
  xkabledply(df[1:rows, ], title, digits, pos, bso, wide=FALSE)
}

xkabledplytail = function(df, rows=5, title="Tail", digits = 4, pos="left", bso="striped") { 
  trows = nrow(df)
  xkabledply(df[ (trows-rows+1) : trows, ], title, digits, pos, bso, wide=FALSE)
}

xkablesummary = function(df, title="Table: Statistics summary.", digits = 4, pos="left", bso="striped") { 
  #' Combining base::summary, xtable, and kableExtra, to easily display numeric variable summary of dataframes. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param df The dataframe.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @return The HTML summary table for display, or for knitr to process into other formats 
  #' @examples
  #' xkablesummary( faraway::ozone )
  #' xkablesummary( ISLR::Hitters, title="Five number summary", pos="left", bso="hover"  )
  
  s = summary(df) %>%
    apply( 2, function(x) stringr::str_remove_all(x,c("Min.\\s*:\\s*","1st Qu.\\s*:\\s*","Median\\s*:\\s*","Mean\\s*:\\s*","3rd Qu.\\s*:\\s*","Max.\\s*:\\s*")) ) %>% # replace all leading words
    apply( 2, function(x) stringr::str_trim(x, "right")) # trim trailing spaces left
  
  colnames(s) <- stringr::str_trim(colnames(s))
  
  if ( dim(s)[1] ==6 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max') 
  } else if ( dim(s)[1] ==7 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max','NA') }
  
  xkabledply(s, title=title, digits = digits, pos=pos, bso=bso )
}

xkablevif = function(model, title="VIFs of the model", digits = 3, pos="left", bso="striped", wide=TRUE) { 
  #' Combining faraway::vif, xtable, and kableExtra, to easily display numeric summary of VIFs for a model. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param model The lm or compatible model object.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @param wide print table in long (FALSE) format or wide (TRUE) format
  #' @return The HTML summary table of the VIFs for a model for display, or for knitr to process into other formats 
  #' @examples
  #' xkablevif( lm(Salary~Hits+RBI, data=ISLR::Hitters), wide=T )
  
  vifs = table( names(model$coefficients)[2:length(model$coefficients)] ) # remove intercept to set column names
  vifs[] = faraway::vif(model) # set the values
  if (wide) { vifs <- t(vifs) }
  xkabledply( vifs, title=title, digits = digits, pos=pos, bso=bso )
}
```

```{r outlierKD2}
# Fix outliers
outlierKD2 <- function(df, var, rm=FALSE) { 
    #' Original outlierKD functino by By Klodian Dhana,
    #' https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/
    #' Modified to have third argument for removing outliers instead of interactive prompt, 
    #' and after removing outlier, original df will not be changed. The function returns a new df, 
    #' which can be saved as original df name if desired.
    #' Check outliers, and option to remove them, save as a new dataframe. 
    #' @param df The dataframe.
    #' @param var The variable in the dataframe to be checked for outliers
    #' @param rm Boolean. Whether to remove outliers or not.
    #' @return The dataframe with outliers replaced by NA if rm==TRUE, or df if nothing changed
    #' @examples
    #' outlierKD2(mydf, height, FALSE)
    #' mydf = outlierKD2(mydf, height, TRUE)
    #' mydfnew = outlierKD2(mydf, height, TRUE)
    dt = df # duplicate the dataframe for potential alteration
    var_name <- eval(substitute(var),eval(dt))
    na1 <- sum(is.na(var_name))
    m1 <- mean(var_name, na.rm = T)
    par(mfrow=c(2, 2), oma=c(0,0,3,0))
    boxplot(var_name, main="With outliers")
    hist(var_name, main="With outliers", xlab=NA, ylab=NA)
    outlier <- boxplot.stats(var_name)$out
    mo <- mean(outlier)
    var_name <- ifelse(var_name %in% outlier, NA, var_name)
    boxplot(var_name, main="Without outliers")
    hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
    title("Outlier Check", outer=TRUE)
    na2 <- sum(is.na(var_name))
    cat("Outliers identified:", na2 - na1, "\n")
    cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "\n")
    cat("Mean of the outliers:", round(mo, 2), "\n")
    m2 <- mean(var_name, na.rm = T)
    cat("Mean without removing outliers:", round(m1, 2), "\n")
    cat("Mean if we remove outliers:", round(m2, 2), "\n")
    
    # response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
    # if(response == "y" | response == "yes"){
    if(rm){
        dt[as.character(substitute(var))] <- invisible(var_name)
        #assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
        cat("Outliers successfully removed", "\n")
        return(invisible(dt))
    } else {
        cat("Nothing changed", "\n")
        return(invisible(df))
    }
}
# sample usage
# mlb2 = outlierKD2(mlb, weight, TRUE) # This will remove weight outliers, replace those values by NA, then save it as a new dataframe mlb2
# mlb = outlierKD2(mlb, weight, TRUE) # This will remove weight outliers, replace those values by NA, then REPLACE the dataframe mlb with the new one.
# outlierKD2(mlb, weight, FALSE) # This will NOT remove weight outliers, but it will show the charts with and without outliers nonetheless. 
# outlierKD2(mlb, weight) # same as above, as the last argument is optional, default = FALSE 
```

![](covid19.png)

# Chapter 1: Introduction

Team A are the following members: Amal Alqahtani, Jiaxiang Peng, Naureen Elahi, and Xinya Mu. You may find our work over on [GitHub](https://github.com/amalqahtani/Data_Science).

Coronavirus disease-19 (COVID-19) has spread rapidly around the world, creating unprecedented damage the world was not ready for. To date, the CDC states there are a total of 4,542, 579 cases and 152, 870 deaths in the United States (Cases in U.S, 2020). Many risk factors have been hypothesized to affect the case and death rates from the virus. We felt that a relevant discussion to have would be `What are the most regions with the highest number of deaths`? What can we say about patient demographics? Is race considered a significant risk factor for increased COVID-19 incidence in the United States?' Are there any general trends amongst underlying health conditions? 
These questions are all suited to Exploratory Data Analysis (EDA), and with these questions in mind, we want to see if we could find data on COVID-19 that would be readily available for us to analyze. Eventually, our question morphed into the following: **What are the factors (i.e. patient demographics, social determinants of health, environmental variables, underlying health conditions, country of origin) affecting COVID-19 numbers of cases and death rate among different geographical locations in US?** 


We were able to find a dataset called `Covid-19-Dataset` on Github over here: [https://github.com/johndurbin93/Covid-19-Dataset](https://github.com/johndurbin93/Covid-19-Dataset). This dataset includes COVID-19 confirmed case number and death number through April 14, 2020  which were obtained for each U.S. county from the Center for Systems Science and Engineering (CSSE) Coronavirus Resource Center at Johns Hopkins University. Race demographics for counties was obtained from the County Health Rankings and Roadmaps Program database. Daily temperature data for counties was obtained from the National Oceanic and Atmospheric Administration. This data was compiled by a group of reserchers. 

The report is organized as follows:

1. Description of the Data (explanation of the dataset and its variables),
2. Demographics Data of the patients
3. Independent Variables EDA: Slicing the Data for an Overview
4. Independent Variables EDA: Boxplots, Scatterplots, ANOVA, & Chi-Square
5. Linear Regression Model
6. Conclusion


# Chapter 2: Description of the Data
## Source Data


The data looks like the following:

```{r input_data}
library("readxl")
data <- read_excel("Counties_Dataset.xlsx")
#summary(data)
str(data)
#names(data)[8:] <- c("total_cases","deaths")
cols <- c(1:2, 7:19, 56)
small_df=data[,cols]
names(small_df)[16] <- c("poor_health")
#Disease_data<-data[,c(1,2,7,8,47,50,53,55,60,61,65,71,72)]
#stay_at_home<-data[,c(1,2,5,7,78,79,80,81,82)]
#str(stay_at_home)
#Print out the first 5 and the last 3 rows of the dataframe.**  
#head(data, 5)
#tail(data, 3)

```

The Covid19 dataset has `r ncol(data)` columns and `r nrow(data)` rows/entries, for a total of `r ncol(data)*nrow(data)` individual data points. Out of `r ncol(data)`, we select the following variables to do EDA:

1. Province
2. State
3. State Code
4. Tests
5. Total cases
6. Deaths
7. Population (for demographic %'s)
8. % less than 18 years of age 
9. % 65 and over
10. % Black
11. % American Indian & Alaska Native
12. % Asian
13. % Native Hawaiian/Other Pacific Islander
14. % Hispanic
15. % Non-Hispanic White
16. % Not Proficient in English
17. % Female
18. No Cases	
19. No Stay At Home Order	
20. Stay At Home Order After First Case
21. Percentage Living in Poverty
22. Social Association Rate

<<<<<<< Updated upstream
=======
47. % Sleep Hour < 7 
48. % Sleep Hour < 7 Confidence Intervial low 
49. % Sleep Hour < 7 Confidence Intervial high
50. % Diabetes Total Percentage 
51. % Diabetes Total Male Percentage
52. % Diabetes Total Female Percentage
53. Coronary Heart Death Rate per 100,000 people  
54. Hyperten slon Death Rate per 100,000 people  
55. % Obesity Age adjuseted 
56. % Fair or Poor Heath
57. Average number of Physically Unhealthy Days 
58. Average number of Mentally Unhealthy Days
59. % Low Birthweight 
60. % Smokers 
61. % Adults with Obesity
62. Food Environment Index 
63. % Physically Inactive 
64. % With Access to Exercise Opportunities 
65. % Excessive Drinking 
66. % Unisured
67. Preventable Hospitalization Rate
68. % With Annual Mammogram
69. % Flu Vaccinated 
70. Chronic Respiratory Disease per 100,000 people
71. Liver Disease: crude mortality per 100,000 people
72. Liver Disease: % of Total death
>>>>>>> Stashed changes

To prepare our data for EDA we clean the dataset and remove all NAs. 
```{r remove_NAs}
#Print the number of NA values per each columns 
#colSums(is.na(data))
#Which col contains NA values? 
#colnames(data)[!complete.cases(t(data))]

#Drops rows containing missing values in any variable:
data_noNA = na.omit(small_df)
str(data_noNA)
```

# Chapter 3:  Independent Variables EDA

## United States COVID-19 Cases and Deaths by Provinces (Cities)


### What are the top 15 Provinces based on the number of cases?

The following bar chart shows the top 15 cities by number of Covid-19 cases. 

```{r top_10_cases, echo=F}
 library(ggplot2)
 library(dplyr)
  top_n(data_noNA, n=15, total_cases) %>%
          ggplot(., aes(x=reorder(Province, -total_cases), y=total_cases, color=Province, fill=Province)) +
  geom_bar(aes(y=total_cases), stat="identity") + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 1, size = 12, hjust = 1), 
        axis.text.y = element_text(vjust = 1, size = 12, hjust = 1), 
        panel.border = element_rect(colour = "black", fill=NA, size=1)) + 
  labs(title="Barchart of Top 15 Provinces (determined by number of Cases)", x="Province", y="Number of Cases")   
  
```

The above Bar chart shows the top 15 provinces determined by the number of cases. New York province is highest city with number of covid19 cases, the total number is over 100000, while the number of cases in other cities  is less than 30000. 


### What are the top 15 Provinces based on the number of deaths?

The following bar chart shows the top 15 cities by number of deaths. 

```{r top_15_deaths, echo=F}
 library(ggplot2)
 library(dplyr)
  top_n(data_noNA, n=15, deaths) %>%
          ggplot(., aes(x=reorder(Province, -deaths), y=deaths, color=Province, fill=Province)) +
  geom_bar(aes(y=deaths), stat="identity") + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 1, size = 12, hjust = 1), 
        axis.text.y = element_text(vjust = 1, size = 12, hjust = 1), 
        panel.border = element_rect(colour = "black", fill=NA, size=1)) + 
  labs(title="Barchart of Top 15 Provinces (determined by number of Deaths)", x="Province", y="Number of Deaths")   
  
```

The above Bar chart shows the top 15 provinces determined by the number of deaths. New York province is highest city with number of deaths around 8000, while the number of deaths in other cities  is less than 1000. 


### What are the top 15 States based on the number of Tests?

```{r testsPerState, echo=F}
testsPerState=distinct(data, State, Tests)
```
```{r top_10_test, echo=F}
 library(ggplot2)
 library(dplyr)
    top_n(testsPerState, n=15, Tests) %>%
          ggplot(., aes(x=reorder(State, -Tests), y=Tests, color=State, fill=State)) +
  geom_bar(aes(y=Tests), stat="identity") + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 1, size = 12, hjust = 1), 
        axis.text.y = element_text(vjust = 1, size = 12, hjust = 1), 
        panel.border = element_rect(colour = "black", fill=NA, size=1)) + 
  labs(title="Barchart of Top 15 States (determined by number of Tests)", x="State", y="Number of Tests")   
  
```

The above Bar chart shows the top 15 States determined by the number of tests. It can be clearly seen that the number of tests has been done in New York State is around 499,143 tests which is considered to be the highest among the other states. Furthermore, the number of test has been done in other states is less than 200k.



### What is the average cases for each State?

```{r rop avg cases, echo=F}
print(aggregate(total_cases~State,data=data_noNA,FUN=mean))
```
### What is the average deaths for each State?

```{r rop avg deaths, echo=F}
print(aggregate(deaths~State,data=data_noNA,FUN=mean))
```


### Which cities had the greatest % of population of people with poor health?

```{r poorhealth, echo=F}
loadPkg("dplyr")
data_noNAinpoorhealth = subset(data_noNA, poor_health != "NA")[,c(1,16)]
data_noNAinpoorhealth$poor_health=as.numeric(data_noNAinpoorhealth$poor_health)
#head(data_noNAinpoorhealth)
a=arrange(data_noNAinpoorhealth, desc(poor_health))
head(a,10)
``` 

```{r cities_poor_health, echo=F}
 library(ggplot2)
 library(dplyr)
    top_n(data_noNAinpoorhealth, n=15, poor_health) %>%
          ggplot(., aes(x=reorder(Province, -poor_health), y=poor_health, color=Province, fill=Province)) +
  geom_bar(aes(y=poor_health), stat="identity") + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 1, size = 12, hjust = 1), 
        axis.text.y = element_text(vjust = 1, size = 12, hjust = 1), 
        panel.border = element_rect(colour = "black", fill=NA, size=1)) + 
  labs(title="Barchart of the top 15 cities had the greatest % of population of people with poor health", x="Province", y="Poor_health")   
  
```


## Patient Demographics
### What are the patient demographics?
```{r}
getwd()
df=data.frame(read.csv("V1.csv", header = TRUE))[,-c(25,26,27,28,29,30,31,32,33)]
#str(df)
colnames(df) <- c("TC","death","Population","young","old","black","AIAN","Asian","NH","Hispanic","NHW","Female","Rural","Population.Density","Housing.Density","Sunlight","GDP","Poverty","Unemployed","Children.Poverty","Income.Inequality","Social","PM2.5","Water","SHP")
df$Water=as.factor(df$Water)
df$Poverty=as.numeric(df$Poverty)
#str(df)

loadPkg("DMwR")
df2=centralImputation(df) #Fill in missing values
# sum(is.na(df2)) #Now there are no missing values

xkablesummary(df2[,c(1,3,4,5,6,7,8,9,10,11,12,18,22)])
```

From the average of the output results, we can see that the average proportion of teenagers under the age of 18 is 22.1%, and the average proportion of people over 65 is 19.3%. The largest number of all races is Non-Hispanic White, with an average proportion of 76.2. The average proportion of women is 49.9, the average proportion of the poor is 15.9%, and the average of the Social Association Rate is 11.6.
We divide the data into four levels according to total cases.

### Which race is the majority of the sample?
```{r}
racemean = data.frame(Race.ratio= c(8.8,2.4,1.5,0.1,9.6,76.2), legend= c('Black','American Indian & Alaska Native','Asian','Native Hawaiian/Other Pacific Islander','Hispanic','Non-Hispanic White'))
p = ggplot(racemean, aes(x = "", y =Race.ratio, fill =legend)) + 
  geom_bar(stat = "identity", width = 1) +  
  coord_polar(theta = "y")   
p
```
According to the average value, we get a pie chart of race proportions, from which we can see the overall proportions of different races. In the following, we will study the proportion of which race is related to the number of confirmed cases and the number of deaths.

## Stay at home policy in each province
### Is the stay at home policy related to the Total Cases infected?
First, we do the t-test and find the 80% and 99% confidence interval.
```{r,t-test,echo=F}
loadPkg("dplyr") 
#remove the outlier for total_cases
stay_at_home<-data[,c(1,2,5,7,78,79,80,81,82)]
names(stay_at_home)[5:9] <- c("first_case","stay_home","no_case","no_order","order_after_first")
stay_at_home$first_case<-as.factor(stay_at_home$first_case)
#bike$Season<-as.factor(bike$Season)
#colnames(stay_at_home)[8,9]=c("no_order","order_after_first")
#stay_at_home<-subset(stay_at_home,is.na(total_cases))
nrow(stay_at_home)
#str(stay_at_home)
#stay_at_home<-stay_at_home %>% subset(total_cases>0)
#nrow(stay_at_home)
stay_at_home
stay_at_home = outlierKD2(stay_at_home, total_cases, TRUE)
nrow(stay_at_home)
Before_order <- subset(stay_at_home,no_order==1)
After_order  <- subset(stay_at_home,order_after_first==1)
Before_ttest80 = t.test(x = Before_order$total_cases, conf.level = 0.80)
Before_ttest80
After_ttest80 = t.test(x = After_order$total_cases, conf.level = 0.80)
After_ttest80
Before_ttest99 = t.test(x = Before_order$total_cases, conf.level = 0.99)
Before_ttest99
After_ttest99 = t.test(x = After_order$total_cases, conf.level = 0.99)
After_ttest99
```
Before first case have stay at home order 80% confidence interval  (8.34,9.87) is not overlap with after first case have stay at home order(22.2,24.0).

Before first case have stay at home order 99% confidence interval  (7,56,10.65) is not overlap with after first case have stay at home order(21.4,24.9).

As a result, the average Total Cases infected number is different from separate groups that the Stay at Home Policy is order before the First Case or not.

### Is the stay at home policy after the first case related to the people inflected number?
```{r,box_plot,echo=F}
ggplot(data = Before_order, aes(x=first_case, y=total_cases))+
  geom_boxplot(fill = "#0000aa", alpha = .7)+
  labs(title="Total Cases Boxplot for Before the  First Case Stay at Home") +
  labs(x="First Cases", y="Total Cases") 
ggplot(data = After_order, aes(x=first_case, y=total_cases))+
  geom_boxplot(fill="#FF9933", alpha = .7)+
  labs(title="Total Cases Boxplot for After the First Case Stay at Home") +
  labs(x="First Cases", y="Total Cases") 
#str(stay_at_home)
```

For these two box-plot diagoram, the Total Cases infected are more higher if the Stay at Home Policy is ordered After the first cases. So we can say the Stay at Home Policy do decrease the Totall Cases inflected if it is ordered before the Fist Case happened. 


## Underlying Health Conditions
### Are there any common underlying health conditions?
### Does any disease relate to the death rate?

```{r,disease,echo=F}
loadPkg("dplyr") 
loadPkg("corrplot")
#str(disease)
disease<-data[,c(1,2,7,8,47,50,53,55,60,61,65,71,72)]
names(disease)[5:13] <- c("sleep_hour","diabetes","heart_disease","obesity_age","smokers",
"adult_obesity","excessive_drink","liver_crude_mortality","liver_Total_death","")
disease2<-disease[ , -which(colnames(disease) %in% c("Province","State","total_cases"))]

disease2$sleep_hour<-as.numeric(as.character(disease2$sleep_hour))
disease2$smokers<-as.numeric(as.character(disease2$smokers))
disease2$adult_obesity<-as.numeric(as.character(disease2$adult_obesity))
disease2$excessive_drink<-as.numeric(as.character(disease2$excessive_drink))
disease2$liver_crude_mortality<-as.numeric(as.character(disease2$liver_crude_mortality))
disease2$liver_Total_death<-as.numeric(as.character(disease2$liver_Total_death))
disease2=disease2 %>% subset(deaths>0)
#disease2=disease2 %>% subset(total_cases>0)
disease2=disease2 %>% subset(heart_disease>0)
disease2=disease2 %>% subset(obesity_age>0)
disease2=disease2 %>% subset(smokers>0)
disease2=disease2 %>% subset(diabetes>0)
disease2=disease2 %>% subset(sleep_hour>0)
disease2=disease2 %>% subset(adult_obesity>0)
disease2=disease2 %>% subset(excessive_drink>0)
disease2=disease2 %>% subset(liver_crude_mortality>0)
disease2=disease2 %>% subset(liver_Total_death>0)
disease2_corr<-cor(disease2)
#str(disease2)
corrplot(disease2_corr,method="circle")
#corrplot.mixed(cor(disease2))
#corrplot.mixed(cor(race))
```


It shows liver_total_death is highly correlated to deaths at correlation = 0.4338.


```{r,disease3,echo=F}
disease4<-data[,c(1,2,7,8,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72)]
names(disease4)[5:30] <- c("sleep_hour","sleep_hour_low","sleep_hour_high","diabetes","diabetes_male","diabetes_female","heart_disease","Hypertension","obesity_age","Poor_health","Physical_Unhealthy","Mentally_unhealthy","Low_birthweight","smokers","adult_obesity","Food_environment","Physically_inactive","Exercise_opportunity","excessive_drink","Uninsured","Preventable_Hospital","Mammogram","Flu_vaccinated","Respiratory","liver_crude_mortality","liver_Total_death")
<<<<<<< Updated upstream
#disease<-data[,c(1,2,7,8,47,50,53,55,60,61,65,71,72)]
=======
>>>>>>> Stashed changes
disease3<-disease4[ , -which(colnames(disease4) %in% c("Province","total_cases"))]
disease3$sleep_hour<-as.numeric(as.character(disease3$sleep_hour))
disease3$sleep_hour_low<-as.numeric(as.character(disease3$sleep_hour_low))
disease3$sleep_hour_high<-as.numeric(as.character(disease3$sleep_hour_high))
disease3$diabetes<-as.numeric(as.character(disease3$diabetes))
disease3$diabetes_male<-as.numeric(as.character(disease3$diabetes_male))
disease3$diabetes_female<-as.numeric(as.character(disease3$diabetes_female))
disease3$heart_disease<-as.numeric(as.character(disease3$heart_disease))
disease3$Hypertension<-as.numeric(as.character(disease3$Hypertension))
disease3$obesity_age<-as.numeric(as.character(disease3$obesity_age))
disease3$Poor_health<-as.numeric(as.character(disease3$Poor_health))
disease3$Physical_Unhealthy<-as.numeric(as.character(disease3$Physical_Unhealthy))
disease3$Mentally_unhealthy<-as.numeric(as.character(disease3$Mentally_unhealthy))
disease3$Low_birthweight<-as.numeric(as.character(disease3$Low_birthweight))
disease3$smokers<-as.numeric(as.character(disease3$smokers))
disease3$adult_obesity<-as.numeric(as.character(disease3$adult_obesity))
disease3$Food_environment<-as.numeric(as.character(disease3$Food_environment))
disease3$Physically_inactive<-as.numeric(as.character(disease3$Physically_inactive))
disease3$Exercise_opportunity<-as.numeric(as.character(disease3$Exercise_opportunity))
disease3$excessive_drink<-as.numeric(as.character(disease3$excessive_drink))
disease3$Uninsured<-as.numeric(as.character(disease3$Uninsured))
disease3$Preventable_Hospital<-as.numeric(as.character(disease3$Preventable_Hospital))
disease3$Mammogram<-as.numeric(as.character(disease3$Mammogram))
disease3$Flu_vaccinated<-as.numeric(as.character(disease3$Flu_vaccinated))
disease3$Respiratory<-as.numeric(as.character(disease3$Respiratory))
disease3$liver_crude_mortality<-as.numeric(as.character(disease3$liver_crude_mortality))
disease3$liver_Total_death<-as.numeric(as.character(disease3$liver_Total_death))
disease3=disease3 %>% subset(deaths>0)
disease3=disease3 %>% subset(sleep_hour_low>0)
disease3=disease3 %>% subset(sleep_hour_high>0)
disease3=disease3 %>% subset(Low_birthweight>0)
disease3=disease3 %>% subset(Food_environment>0)
<<<<<<< Updated upstream
#disease3=disease3 %>% subset(deaths>0)
#disease2=disease2 %>% subset(total_cases>0)
=======
>>>>>>> Stashed changes
disease3=disease3 %>% subset(heart_disease>0)
disease3=disease3 %>% subset(obesity_age>0)
disease3=disease3 %>% subset(smokers>0)
disease3=disease3 %>% subset(diabetes>0)
disease3=disease3 %>% subset(sleep_hour>0)
disease3=disease3 %>% subset(adult_obesity>0)
disease3=disease3 %>% subset(excessive_drink>0)
disease3=disease3 %>% subset(liver_crude_mortality>0)
disease3=disease3 %>% subset(liver_Total_death>0)
disease5<-disease3[ , -which(colnames(disease3) %in% c("State"))]
disease5_corr<-cor(disease5)
<<<<<<< Updated upstream
#str(disease2)
corrplot(disease5_corr,method="circle")
=======
corrplot(disease5_corr,method="circle")
#corrplot(disease5_corr,method="circle")
corrplot.mixed(cor(disease5))
>>>>>>> Stashed changes
```


```{r,feasure,echo=F}
loadPkg("leaps")
reg.leaps <- regsubsets(deaths~., data = disease5, nbest = 1, method = "exhaustive")  # leaps, 
plot(reg.leaps, scale = "bic", main = "BIC")
plot(reg.leaps, scale = "adjr2", main = "Adjusted R^2")
lm.dis5<-lm(deaths~sleep_hour+sleep_hour_high+Low_birthweight+heart_disease+smokers+adult_obesity+Food_environment+Respiratory+liver_Total_death,data = disease3)
summary(lm.dis5)
faraway::vif(lm.dis5)
lm.dis4<-lm(deaths~sleep_hour++Low_birthweight+heart_disease+adult_obesity+Food_environment+Respiratory+liver_Total_death,data = disease5)
summary(lm.dis4)
faraway::vif(lm.dis4)
```

## Impact of Temperature

### Does the temperature relate the Total Cases or Death Rate? 

```{r,temperature,echo=F}
temp<-data[,c(1,2,6,7,8,75,76,77)]
names(temp)[3] <- c("days")
names(temp)[6:8] <- c("temp_peak","temp_before","temp_current")
str(temp)
<<<<<<< Updated upstream
#ggplot(data=temp)+
#  geom_point(mapping = aes(x=temp_peak, y=total_cases, color=State))+
#  ggtitle("Scatter plot of baseball player weigth(y, lbs) vs height(x, inches)")
temp2<-temp[ , -which(colnames(temp) %in% c("Province","State"))]
#temp2$liver_Total_death<-as.numeric(as.character(disease2$liver_Total_death))
=======
temp2<-temp[ , -which(colnames(temp) %in% c("Province","State"))]
>>>>>>> Stashed changes
temp2=temp2 %>% subset(deaths>0)
temp2=temp2 %>% subset(days>0)
temp2=temp2 %>% subset(total_cases>0)
temp2=temp2 %>% subset(temp_peak>0)
temp2=temp2 %>% subset(temp_before>0)
temp2=temp2 %>% subset(temp_current>0)
temp2_corr<-cor(temp2)
<<<<<<< Updated upstream
#str(disease2)
corrplot(temp2_corr,method="circle")
```

By the correlation diagorm, the temperature is less relate to total_cases and deaths. 
=======
corrplot(temp2_corr,method="circle")
corrplot.mixed(cor(temp2))
```

By the correlation diagorm, the temperature is less relate to total_cases and deaths. However, it is relate to the days. Which means the higher the tempeture in the city, the later the disese occurs in this city.
>>>>>>> Stashed changes


# Chapter 4: Independent Variables EDA: Boxplots, Scatterplots, ANOVA, & Chi-Square

First, we divide the total cases into different levels according to the quartile value and the median for further analysis.
```{r}
#sum(is.na(df2$TC))
break1=fivenum(df2$TC)
break1
labels = c("0-2", "2-9", "9-39", "39-110465")
rank=cut(df2$TC,break1,labels,ordered_result = T)

shapiro.test(df2$TC)
bartlett.test(TC ~ rank, data=df2)
```
The Shapiro-Wilk test is used to test whether the data conforms to the normal distribution.

H0: The sample data is not significantly different from the normal distribution

H1: The sample data is significantly different from the normal distribution

The p-value is less than 0.05, the null hypothesis is rejected, and the total cases do not conform to the normal distribution.

Test for homogeneity of variance(Bartlett test)

H0: Data with the same variance at different levels

H1: Data without the same variance at different levels

The result shows that the p value is less than 0.05, rejecting the null hypothesis, and total cases do not meet the homogeneity of variance.

## SMART Question: The proportion of which race is related to the number of confirmed cases and deaths
```{r}
race=subset(df2[,c(1,2,6,7,8,9,10,11)])
loadPkg("corrplot")
cor(race)
corrplot.mixed(cor(race))
```


First, let's look at the relationship between the total number of cases and race. It can be seen from the correlation coefficient graph that the positive correlation coefficient between total cases and Asians is the largest, which indicates that the higher the proportion of Asians, the more total cases, and Asian infections may be more serious. The other two races with higher correlation coefficients with TC are Hispanics and blacks. The correlation coefficient between total cases and non-Hispanic whites is negative, indicating that the higher the proportion of this race, the fewer the number of people diagnosed with COVID-19.

Second, we can see the relationship between the number of deaths and race, and the results are consistent with the above results.

## SMART Question: Are the total cases related to age?
### Are the total cases related to the proportion of people over 65?
```{r boxplot old, echo=FALSE}
loadPkg("plotly")
ggplotly(ggplotly(ggplot(df2, aes(x=rank, y=old, fill=rank)) + geom_boxplot() + scale_fill_brewer(palette="Spectral") + ggtitle("Proportion of elderly over 65 vs. Total cases") + ylab("Proportion of elderly over 65") + xlab("Total cases") +  theme(plot.title= element_text(hjust=0.5, size = 14))))
```


It can be seen from the image that the proportion of elderly people is lower in areas with more total cases.

```{r old test}
loadPkg('lmtest')
test1 <- bptest(df2$old~rank)
test1$p.value

old.anova <- aov(df2$old~rank)
summary(old.anova)
TKcond <- TukeyHSD(old.anova)
TKcond

par(las=1)
plot(TukeyHSD(old.anova))
```

According to the BP test, the p value is greater than 0.05, and the null hypothesis is accepted: the variance of old is the same under different ranks. The next step ANOVA test can be performed.

According to the ANOVA test, the p value is less than 0.05, and the null hypothesis is rejected: the mean value of old is the same under different ranks. It shows that the proportion of elderly people over 65 years old is different in regions with different confirmed cases.

According to the Tukey HSD test for multiple comparisons, all p values are less than 0.05, so the difference in old between different ranks is statistically significant.

### Are the total cases related to the proportion of young people under 18?

```{r boxplot young, echo=FALSE}
ggplotly(ggplotly(ggplot(df2, aes(x=rank, y=young, fill=rank)) + geom_boxplot() + scale_fill_brewer(palette="Spectral") + ggtitle("proportion of youth under 18 vs. Total cases") + ylab("proportion of youth under 18") + xlab("Total cases") +  theme(plot.title= element_text(hjust=0.5, size = 14))))
```

It is difficult to see whether the two are related from the image, so we will examine it next. The first is the bp test to see if the hypothesis of the ANOVA test is satisfied. If it is satisfied, we will perform an ANOVA test. If it is not satisfied, we will perform a chi-square test.

```{r young bptest,echo=F}
test2 <- bptest(df2$young~rank)
test2$p.value
```
Because the p-value is less than 0.05, indicating that the homoscedasticity assumption is not satisfied, we cannot apply the ANOVA test. Next, we will also convert the young variable into a categorical variable and perform a chi-square test.

H0: The total cases and number of Percentage of young people under 18 are independent.  
H1: The total cases and number of Percentage of young people under 18 are not independent.
```{r young chitest}
break2=fivenum(df2$young)
break2
labels = c("0-20.1", "20.1-22.1", "22.1-23.8", "23.8-42.0")
youncate=cut(df2$young,break2,labels,ordered_result = T)

young_p <- table(rank, youncate)
x2test1 = chisq.test(young_p)
x2test1

```
The p value is less than 0.05, and the null hypothesis is rejected at the significance level of 0.05, indicating that there is a relationship between the two variables. The percentage of people under 18 will affect the total cases.

## SMART Question: Are the total cases related to gender?
```{r}
ggplotly(ggplotly(ggplot(df2, aes(x=rank, y=Female, fill=rank)) + geom_boxplot() + scale_fill_brewer(palette="Spectral") + ggtitle("Female vs. Total cases") + ylab("Proportion of female") + xlab("Total cases") +  theme(plot.title= element_text(hjust=0.5, size = 14))))
```

In areas with more total cases, the average percentage of women is higher.
```{r gender bptest}
test3 <- bptest(df2$Female~rank)
test3$p.value
```

```{r gender chitest}
break3=fivenum(df2$Female)
break3
labels = c("26.8-49.4", "49.4-50.3", "50.3-51.0", "51.0-56.9")
femalecate=cut(df2$Female,break3,labels,ordered_result = T)

female_p <- table(rank, femalecate)
x2test2 = chisq.test(female_p)
x2test2

```

From the results of the bp test, it can be seen that the p value is less than 0.05, the null hypothesis is rejected, and the hypothesis of same variance is not satisfied, so the chi-square test is performed. The chi-square test p-value is less than 0.05, rejecting the null hypothesis that these two variables are independent of each other. Therefore, the proportion of female is related to total cases.

## SMART Question: Are the total cases related to Poverty?
```{r}
ggplotly(ggplotly(ggplot(df2, aes(x=rank, y=Poverty, fill=rank)) + geom_boxplot() + scale_fill_brewer(palette="Spectral") + ggtitle("Poverty vs. Total cases") + ylab("Proportion of Poverty") + xlab("Total cases") +  theme(plot.title= element_text(hjust=0.5, size = 14))))
```

From the first to the third level, the more total cases, the larger the average proportion of poor people. The average proportion of poor people in severely affected areas is the lowest.

```{r Poverty bptest}
test4 <- bptest(df2$Poverty~rank)
test4$p.value


Poverty.anova <- aov(df2$Poverty~rank)
summary(Poverty.anova)
TKcond <- TukeyHSD(Poverty.anova)
TKcond

par(las=1)
plot(TukeyHSD(Poverty.anova))
```


The p value of bp test is greater than 0.05, which meets the premise of ANOVA test. The p value of the ANOVA test is less than 0.05, and the null hypothesis is rejected: the mean value of Poverty is the same under different ranks, indicating that the total cases of different levels have different preference. From the results of Tukey HSD, we can see that the difference between total cases from 0 to 39 is not big, but the fourth level is significantly different from other levels. It shows that where the epidemic is severe, the poverty is obviously different from other places.


# Chapter 5: Linear Regression Model
## SMART Question: What factors influence the death rate the most? 

## Which disease variables best explian death rate?


Use exhaustive method for feature selection 
```{r,q10,echo=F}
loadPkg("leaps")
reg.leaps <- regsubsets(deaths~., data = disease2, nbest = 1, method = "exhaustive")  # leaps, 
plot(reg.leaps, scale = "bic", main = "BIC")
plot(reg.leaps, scale = "adjr2", main = "Adjusted R^2")

lm.dis1<-lm(deaths~sleep_hour+obesity_age+liver_crude_mortality+liver_Total_death,data = disease2)
summary(lm.dis1)
faraway::vif(lm.dis1)

lm.dis2<-lm(deaths~sleep_hour+heart_disease+smokers+adult_obesity+liver_crude_mortality+liver_Total_death,data = disease2)
summary(lm.dis2)
faraway::vif(lm.dis2)
```
For BIC model selection: sleep_hour, obesity_age, liver_crude_mortality and liver_total_death are best variable selection.

ALL P_values <<0.05 and VIF values all < 5, good model.

For Adjusted R^2: Sleep_hour, heart_disease, smokers, adult_obesity, liver_crude_mortality and liver_total_death are best variable selection 

ALL P_values <<0.05 and VIF values all < 5, good model.

```{r,cp,echo=F}
loadPkg("carData")
loadPkg("car")
layout(matrix(1:2, ncol = 2))
## Adjusted R2
#res.legend <-
#    subsets(reg.leaps, statistic="adjr2", legend = FALSE, min.size = 3, main = "Adjusted R^2")
## Mallow Cp
res.legend <-
    subsets(reg.leaps, statistic="cp", legend = FALSE, min.size=1, main = "Mallow Cp")
abline(a = 1, b = 1, lty = 2)
    subsets(reg.leaps, statistic="cp", legend = FALSE, min.size=6, main = "Mallow Cp")
abline(a = 1, b = 1, lty = 2)

```


By Mallow Cp line, sleep_hour, heart_disease, smokers, adult_obseity, liver_crude_mortality, liver_total_death are best selection, which is exactly Adjusted R^2 model. 

## Which variables best explian death rate?
```{r}
getwd()
lineardf<-data[,c(1,2,7,8,9,10,11,12,13,14,15,16,17,19,20,21,22,23,24,26,33,34,35,37,39,41,47, 50,53,55,56,58,60,61,65,71,72)]
colnames(lineardf) <- c("Province","State","TC","deaths","population","young","old","black","AIAN","Asian","NH","Hispanic","NHW","Female","Rural","Population.Density","Housing.Density","Sunlight","GDP","Poverty","Unemployed","Children.Poverty","Income.Inequality","Social","PM2.5","SHP","sleep_hour","diabetes","heart_disease","obesity_age", "poorhealth","Unhealthy.Days","smokers","adult_obesity","excessive_drink","liver_crude_mortality","liver_Total_death")
<<<<<<< Updated upstream
#str(lineardf)
lineardf2=as.data.frame(lapply(lineardf,as.numeric))
#lineardf2$Water=as.factor(lineardf2$Water)
# subset(lassodf, TC != "NA")
lineardf3=na.omit(lineardf2[,-c(1,2)])
#str(lineardf3)

=======
lineardf2=as.data.frame(lapply(lineardf,as.numeric))
lineardf3=na.omit(lineardf2[,-c(1,2)])
>>>>>>> Stashed changes
```
```{r,q10__,echo=F}
loadPkg("leaps")
reg.leaps1 <- regsubsets(deaths~., data = lineardf3, nbest = 1, method = "exhaustive")  # leaps, 
plot(reg.leaps1, scale = "bic", main = "BIC")
plot(reg.leaps1, scale = "adjr2", main = "Adjusted R^2")

lm.whole1<-lm(deaths~population+Asian+liver_Total_death,data = lineardf3)
summary(lm.whole1)
faraway::vif(lm.whole1)

lm.whole2<-lm(deaths~population+Asian+Hispanic+liver_Total_death,data = lineardf3)
summary(lm.whole2)
faraway::vif(lm.whole2)

lm.whole3<-lm(deaths~population+Asian,data = lineardf3)
summary(lm.whole3)
faraway::vif(lm.whole3)
```

We use all variables to fit a linear model and select variables based on BIC. The linear model we get contains variables: population, Asian, liver_Total_death. According to the adjusted R-squared variable selection, our model includes: population, Asian, Hispanic, liver_Total_death. In the above two models, only the p value of population and Asian variables is less than 0.05, which is significant, and the VIF of population and liver_Total_death is too large and highly autocorrelated. So we remove liver_Total_death and use the other two variables to fit the third model.

Final model:
death=0.0000493 population + 1.2190622 Asian
Adjusted R-squared=0.217, the model can explain 21.7% of the variation in death.

<<<<<<< Updated upstream
=======
part2: Whch disease variable best explain the death rate? 
```{r,feasure,echo=F}
reg.leaps <- regsubsets(deaths~., data = disease5, nbest = 1, method = "exhaustive")  # leaps, 
plot(reg.leaps, scale = "bic", main = "BIC")
plot(reg.leaps, scale = "adjr2", main = "Adjusted R^2")
lm.dis5<-lm(deaths~sleep_hour+sleep_hour_high+Low_birthweight+heart_disease+adult_obesity+Food_environment+Respiratory+liver_Total_death,data = disease3)
summary(lm.dis5)
faraway::vif(lm.dis5)
lm.dis4<-lm(deaths~sleep_hour++Low_birthweight+heart_disease+adult_obesity+Food_environment+Respiratory+liver_Total_death,data = disease5)
summary(lm.dis4)
faraway::vif(lm.dis4)
```
We use all variables to fit a linear model and select variables based on BIC and adjusted R-squared. The linear model we get contains variables: Sleep_hour, Sleep_hour_high, Heart_disease, Low_birthweight, adult_obesity, Food_enronment, Respiratory and liver_Total_death. In the above two models, all the p value of variables is less than 0.05, which is significant, but the VIF of skeep_hour_high and sleep_hour is too large and highly autocorrelated. So we remove sleep_hour_high and use the other variables to fit the third model.
```{r,cp,echo=F}
loadPkg("carData")
loadPkg("car")
layout(matrix(1:2, ncol = 2))
## Adjusted R2
#res.legend <-
#    subsets(reg.leaps, statistic="adjr2", legend = FALSE, min.size = 3, main = "Adjusted R^2")
## Mallow Cp
res.legend <-
    subsets(reg.leaps, statistic="cp", legend = FALSE, min.size=1, main = "Mallow Cp")
abline(a = 1, b = 1, lty = 2)
    subsets(reg.leaps, statistic="cp", legend = FALSE, min.size=6, main = "Mallow Cp")
abline(a = 1, b = 1, lty = 2)

```
By the Mallo Cp diagorm, the left subset size is 1 and right subset size is 6. When variables reach to seven the Cp-value reach the lowest point around. And the variables are exactly the same as we selected above. 
>>>>>>> Stashed changes

# Chapter 6: LASSO & Ridge Regression

We convert the two variables Presence of Drinking Water Violation and Stay At Home Order After First Case into categorical variables. Then began to fit LASSO regression and ridge regression. We normalize the data, then split the data into training and test set, so that we can estimate test errors. The split will be used here for Lasso and later for Ridge regression. For brevity, we selected 34 variables for the following analysis.

## LASSO Regression

We draw the plot for different $\lambda$ values to see the overall trend.
```{r}
getwd()
lassodf=data.frame(read.csv("V1.csv", header = TRUE))[,-2]

colnames(lassodf) <- c("TC","population","young","old","black","AIAN","Asian","NH","Hispanic","NHW","Female","Rural","Population.Density","Housing.Density","Sunlight","GDP","Poverty","Unemployed","Children.Poverty","Income.Inequality","Social","PM2.5","Water","SHP","poorhealth","Unhealthy.Days","smokers","Obesity","Physically.ina","WAEO","CRD","Temp","Order")


lassodf$Poverty=as.numeric(lassodf$Poverty)
lassodf$Water=as.factor(lassodf$Water)
lassodf$Order=as.factor(lassodf$Order)
#str(lassodf)

# subset(lassodf, TC != "NA")
lassodf=na.omit(lassodf)

```

```{r uzscale_fcn}
uzscale <- function(df, append=0, excl=NULL) { 
  #' Standardize dataframe to z scores, safe for non-numeric variables. 
  #' ELo 201904 GWU DATS
  #' @param df The dataframe.
  #' @param append T/F or 0/1. Option to append scaled columns or replace original columns in the dataframe.
  #' @param excl A list c(a,b,"d","ef") of excluded columns, either by their indexes and/or names.
  #' @return The transformed dataframe, appended or replaced with standardized scores. Non-numeric columns will not be appended, or if "replace option" is chosen, the columns will be untouched.
  #' @examples
  #' library("ISLR")
  #' tmp = uzscale( Hitters )
  #' tmp = uzscale( Hitters, 1 )
  #' tmp = uzscale( Hitters, TRUE, c(19,"NewLeague") )
  
  append = ifelse(append==TRUE || append=="true" || append=="True" || append=="T" || append=="t" || append==1 || append=="1", TRUE, FALSE) # standardize append 
  nmax = length(df)
  if (nmax < 1 || !is.numeric(nmax) ) { return(df) }
  df1 = df
  onames = colnames(df)  # the original column names
  cnames = onames  # the new column names, if needed start with the original ones
  znames = paste("z",cnames, sep="")     # new column names added prefix 'z'. Those are non-numeric will not be used.
  nadd = ifelse(append, nmax, 0) # add to the column index or replace the orig columns
  j=1  # counting index
  for( i in 1:nmax ) {
    if ( is.numeric(df[,i]) && !( i %in% excl || onames[i] %in% excl ) ) { 
      df1[,j+nadd] = scale(df[,i])
      cnames = c(cnames, znames[i])
      j=j+1
    } else if ( !append ) { j=j+1
    } # if append == 1 and (colunm non-numeric or excluded), do not advance j.
  }
  if (append) { colnames(df1) <- cnames }
  return(df1)
}
```

```{r sd&split}
stdf=uzscale(lassodf, 0)

x=model.matrix(TC~.,stdf)[,-1]
y=stdf$TC

loadPkg("glmnet")
grid=10^seq(5,-5,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments

loadPkg("dplyr")
set.seed(1)
train = stdf %>% sample_frac(0.5)
test = stdf %>% setdiff(train)

x_train = model.matrix(TC~., train)[,-1]
x_test = model.matrix(TC~., test)[,-1]

y_train = train %>% select(TC) %>% unlist() # %>% as.numeric()
y_test = test %>% select(TC) %>% unlist() # %>% as.numeric()
```

```{r lassomodel}
lasso.mod=glmnet(x_train,y_train,alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out.lasso=cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out.lasso)
```

```{r}
bestlam.lasso=cv.out.lasso$lambda.min
cat("lowest lamda from CV: ", bestlam.lasso, "\n\n")
```

We see that the lowest MSE is when $\lambda$ appro = `r round(bestlam.lasso,digits=6)`. 

```{r}
lasso.pred=predict(lasso.mod,s=bestlam.lasso,newx=x_test)
#
out.lasso = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lassoMeanMse = mean((lasso.pred-y_test)^2)
cat("Mean MSE for best Lasso lamda: ", lassoMeanMse, "\n\n")
#
lasso_coef = predict(out.lasso, type = "coefficients", s = bestlam.lasso)[1:13,] # Display coefficients using λ chosen by CV
cat("\nAll the coefficients : \n")
lasso_coef
cat("\nThe non-zero coefficients : \n")
lasso_coef[lasso_coef!=0]
```
From LASSO regression, the coefficients of 11 variables are not zero, the coefficients of the remaining variables become zero. From the results, we can see that race, gender, age, population, population density and rural proportions will all have an impact on total cases.

```{r lasso R2, echo=FALSE}
sst1 <- sum((y_test - mean(y_test))^2)
sse1 <- sum((lasso.pred - y_test)^2)
rsq1 <- 1 - sse1 / sst1
```
We then calculate the R squared of lasso regression, which is `r rsq1`.  

## Ridge Regression
```{r}
grid2=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid2)
dim(coef(ridge.mod))
plot(ridge.mod) 
```
```{r}
#ridge.mod$lambda[50] # 11498
coef(ridge.mod)[,50]
#sqrt(sum(coef(ridge.mod)[-1,50]^2)) # 0.000138
#ridge.mod$lambda[60] # 705
coef(ridge.mod)[,60] 
#sqrt(sum(coef(ridge.mod)[-1,60]^2))  # 0.00224

```
Because the ridge regression uses the "L2 norm", the coefficients are expected to be smaller when $\lambda$ is large. Our "mid-point" (the 50-th) of $\lambda$ equals to 11498, and the sum of squares of coefficients = `r round(sqrt(sum(coef(ridge.mod)[-1,50]^2)),digits=10)`. Compared to the 60-th value (we have a decreasing sequence) $\lambda$ of = 705, we find the sum of squares of the coefficients to be `r round(sqrt(sum(coef(ridge.mod)[-1,60]^2)),digits=7)`, about 16 times larger.

We can use the predict function for various purposes, such as getting the predicted coefficients for $\lambda$=50, for example.

```{r predridge, echo=FALSE}
predict(ridge.mod,s=50,type="coefficients")[1:33,]
```

Then we use the separated training set and test set to see the test error.
```{r predrid, echo=FALSE}
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x_test)
```
The test set mean squared error (MSE) is `r mean((ridge.pred-y_test)^2)`. (We are using standardized scores for $\lambda = 4$.)

```{r mseridge, include=FALSE}
mean((mean(y_train)-y_test)^2) # the test set MSE
```
On the other hand, for the null model ($\lambda$ approaches infinity), the MSE can be found to be `r mean((mean(y_train)-y_test)^2)`. So $\lambda = 4$ reduces the variance by about half, at the expense of bias.

```{r mseridge2, echo=FALSE}
ridge.pred=predict(ridge.mod,s=1e10,newx=x_test)
```
We could have also used a large $\lambda$ value to find the MSE for the null model. These two methods yield essentially the same answer of `r mean((ridge.pred-y_test)^2)`.

```{r mseridg3, echo=FALSE}
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
#mean((ridge.pred - y_test)^2)
predict(ridge.mod, s = 0, type="coefficients")[1:33,]
```

Now for the other extreme special case of small $\lambda$, which is the ordinary least square (OLS) model. We can first use the ridge regression result to predict the $\lambda$ =0 case. The MSE was found to be `r mean((ridge.pred - y_test)^2)` using this result. 

We can also build the OLS model directly, caculate MSE.
```{r, echo=FALSE}
ols.mod = lm(TC~., data = train)
summary(ols.mod)
```
The MSE for OLS regression is `r mean(residuals(ols.mod)^2)`


# Chapter 7: Conclusion
# Chapter 8: Bibliography
Cases in the U.S. (2020, August 01). Retrieved August 01, 2020, from     https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/cases-in-us.html 

