
---
title: "Covid19 Data"
author: "Team A"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:  
    rmdformats::readthedown:
      toc_float: true
      number_sections: true
      code_folding: hide
      keep_tex: yes
---



```{r setup, echo=FALSE, cache=FALSE}
  library(knitr)
library(rmdformats)
## Global options
options(max.print="75", scipen = 999, digits = 3, big.mark=",", warn = -1)
opts_chunk$set(echo=FALSE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

```{r basicfunct, include=FALSE}
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```

```{r libraries}
#library(ggplot2)
#require(gridExtra)
#library(kableExtra)
loadPkg("ggplot2")
loadPkg("gridExtra")
loadPkg("kableExtra")
```

```{r xkablesummary}
loadPkg("xtable")
loadPkg("kableExtra")
loadPkg("stringi")

xkabledply = function(modelsmmrytable, title="Table", digits = 4, pos="left", bso="striped", wide=FALSE) { 
  #' Combining base::summary, xtable, and kableExtra, to easily display model summary. 
  #' wrapper for the base::summary function on model objects
  #' Can also use as head for better display
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param modelsmmrytable This can be a generic table, a model object such as lm(), or the summary of a model object summary(lm()) 
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @param wide print table in long (FALSE) format or wide (TRUE) format
  #' @return HTML table for display
  #' @examples
  #' library("xtable")
  #' library("kableExtra")
  #' xkabledply( df, title="Table testing", pos="left", bso="hover" )
  #' xkabledply( ISLR::Hitters[1:5,] )
  if (wide) { modelsmmrytable <- t(modelsmmrytable) }
  modelsmmrytable %>%
    xtable() %>% 
    kable(caption = title, digits = digits) %>%
    kable_styling(bootstrap_options = bso, full_width = FALSE, position = pos)
}

xkabledplyhead = function(df, rows=5, title="Head", digits = 4, pos="left", bso="striped") { 
  xkabledply(df[1:rows, ], title, digits, pos, bso, wide=FALSE)
}

xkabledplytail = function(df, rows=5, title="Tail", digits = 4, pos="left", bso="striped") { 
  trows = nrow(df)
  xkabledply(df[ (trows-rows+1) : trows, ], title, digits, pos, bso, wide=FALSE)
}

xkablesummary = function(df, title="Table: Statistics summary.", digits = 4, pos="left", bso="striped") { 
  #' Combining base::summary, xtable, and kableExtra, to easily display numeric variable summary of dataframes. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param df The dataframe.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @return The HTML summary table for display, or for knitr to process into other formats 
  #' @examples
  #' xkablesummary( faraway::ozone )
  #' xkablesummary( ISLR::Hitters, title="Five number summary", pos="left", bso="hover"  )
  
  s = summary(df) %>%
    apply( 2, function(x) stringr::str_remove_all(x,c("Min.\\s*:\\s*","1st Qu.\\s*:\\s*","Median\\s*:\\s*","Mean\\s*:\\s*","3rd Qu.\\s*:\\s*","Max.\\s*:\\s*")) ) %>% # replace all leading words
    apply( 2, function(x) stringr::str_trim(x, "right")) # trim trailing spaces left
  
  colnames(s) <- stringr::str_trim(colnames(s))
  
  if ( dim(s)[1] ==6 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max') 
  } else if ( dim(s)[1] ==7 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max','NA') }
  
  xkabledply(s, title=title, digits = digits, pos=pos, bso=bso )
}

xkablevif = function(model, title="VIFs of the model", digits = 3, pos="left", bso="striped", wide=TRUE) { 
  #' Combining faraway::vif, xtable, and kableExtra, to easily display numeric summary of VIFs for a model. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param model The lm or compatible model object.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @param wide print table in long (FALSE) format or wide (TRUE) format
  #' @return The HTML summary table of the VIFs for a model for display, or for knitr to process into other formats 
  #' @examples
  #' xkablevif( lm(Salary~Hits+RBI, data=ISLR::Hitters), wide=T )
  
  vifs = table( names(model$coefficients)[2:length(model$coefficients)] ) # remove intercept to set column names
  vifs[] = faraway::vif(model) # set the values
  if (wide) { vifs <- t(vifs) }
  xkabledply( vifs, title=title, digits = digits, pos=pos, bso=bso )
}
```

![](covid19.png)

# Chapter 1: Introduction

Team A are the following members: Amal Alqahtani, Jiaxiang Peng, Naureen Elahi, and Xinya Mu. You may find our work over on [GitHub](https://github.com/amalqahtani/Data_Science).

Coronavirus disease-19 (COVID-19) has spread rapidly around the world, creating unprecedented damage the world was not ready for. To date, the CDC states there are a total of 4,542, 579 cases and 152, 870 deaths in the United States (Cases in U.S, 2020). Many risk factors have been hypothesized to affect the case and death rates from the virus. We felt that a relevant discussion to have would be `What are the most regions with the highest number of deaths`? What can we say about patient demographics? Is race considered a significant risk factor for increased COVID-19 incidence in the United States?' Are there any general trends amongst underlying health conditions? 
These questions are all suited to Exploratory Data Analysis (EDA), and with these questions in mind, we want to see if we could find data on COVID-19 that would be readily available for us to analyze. Eventually, our question morphed into the following: **What are the factors (i.e. patient demographics, social determinants of health, environmental variables, underlying health conditions, country of origin) affecting COVID-19 numbers of cases and death rate among different geographical locations in US?** 


We were able to find a dataset called `Covid-19-Dataset` on Github over here: [https://github.com/johndurbin93/Covid-19-Dataset](https://github.com/johndurbin93/Covid-19-Dataset). This dataset includes COVID-19 confirmed case number and death number through April 14, 2020  which were obtained for each U.S. county from the Center for Systems Science and Engineering (CSSE) Coronavirus Resource Center at Johns Hopkins University. Race demographics for counties was obtained from the County Health Rankings and Roadmaps Program database. Daily temperature data for counties was obtained from the National Oceanic and Atmospheric Administration. This data was compiled by a group of reserchers. 

The report is organized as follows:

1. Description of the Data (explanation of the dataset and its variables),
2. Demographics Data of the patients
3. Independent Variables EDA: Slicing the Data for an Overview
4. Independent Variables EDA: Boxplots, Scatterplots, ANOVA, & Chi-Square
5. Linear Regression Model
6. Conclusion


# Chapter 2: Description of the Data
## Source Data


The data looks like the following:

```{r input_data}
library("readxl")
data <- read_excel("Counties_Dataset.xlsx")
#summary(data)
str(data)
#names(data)[8:] <- c("total_cases","deaths")
cols <- c(1:2, 7:19, 56)
small_df=data[,cols]
names(small_df)[16] <- c("poor_health")
small_df
#Print out the first 5 and the last 3 rows of the dataframe.**  
#head(data, 5)
#tail(data, 3)

```

The Covid19 dataset has `r ncol(data)` columns and `r nrow(data)` rows/entries, for a total of `r ncol(data)*nrow(data)` individual data points. Out of `r ncol(data)`, we select the following variables to do EDA:

1. Province
2. State
3. State Code
4. Tests
5. Total cases
6. Deaths
7. Population (for demographic %'s)
8. % less than 18 years of age 
9. % 65 and over
10. % Black
11. % American Indian & Alaska Native
12. % Asian
13. % Native Hawaiian/Other Pacific Islander
14. % Hispanic
15. % Non-Hispanic White
16. % Not Proficient in English
17. % Female
18. No Cases	
19. No Stay At Home Order	
20. Stay At Home Order After First Case
21. Percentage Living in Poverty
22. Social Association Rate


To prepare our data for EDA we clean the dataset and remove all NAs. 
```{r remove_NAs}
#Print the number of NA values per each columns 
#colSums(is.na(data))
#Which col contains NA values? 
#colnames(data)[!complete.cases(t(data))]

#Drops rows containing missing values in any variable:
data_noNA = na.omit(small_df)
str(data_noNA)
```

# Chapter 3:  Independent Variables EDA

## United States COVID-19 Cases and Deaths by Provinces (Cities)


### What are the top 15 Provinces based on the number of cases?

The following bar chart shows the top 15 cities by number of Covid19 cases. 

```{r top_10_cases, echo=F}
 library(ggplot2)
 library(dplyr)
  top_n(data_noNA, n=15, total_cases) %>%
          ggplot(., aes(x=reorder(Province, -total_cases), y=total_cases, color=Province, fill=Province)) +
  geom_bar(aes(y=total_cases), stat="identity") + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 1, size = 12, hjust = 1), 
        axis.text.y = element_text(vjust = 1, size = 12, hjust = 1), 
        panel.border = element_rect(colour = "black", fill=NA, size=1)) + 
  labs(title="Barchart of Top 15 Provinces (determined by number of Cases)", x="Province", y="Number of Cases")   
  
```


### What are the top 15 Provinces based on the number of deaths?

The following bar chart shows the top 15 cities by number of deaths. 

```{r top_15_deaths, echo=F}
 library(ggplot2)
 library(dplyr)
  top_n(data_noNA, n=15, deaths) %>%
          ggplot(., aes(x=reorder(Province, -deaths), y=deaths, color=Province, fill=Province)) +
  geom_bar(aes(y=deaths), stat="identity") + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 1, size = 12, hjust = 1), 
        axis.text.y = element_text(vjust = 1, size = 12, hjust = 1), 
        panel.border = element_rect(colour = "black", fill=NA, size=1)) + 
  labs(title="Barchart of Top 15 Provinces (determined by number of Deaths)", x="Province", y="Number of Deaths")   
  
```

### What is the average cases for each State?

```{r rop avg cases, echo=F}
print(aggregate(total_cases~State,data=data_noNA,FUN=mean))
```
### What is the average deaths for each State?

```{r rop avg deaths, echo=F}
print(aggregate(deaths~State,data=data_noNA,FUN=mean))
```

### Which cities had the greatest % of population of people with poor health?

## Patient Demographics
### What are the patient demographics?
```{r}
getwd()
df=data.frame(read.csv("V1.csv", header = TRUE))
df <- subset(df[,-c(1,28)])
colnames(df) <- c("TC","death","Population","young","old","black","AIAN","Asian","NH","Hispanic","NHW","Female","Rural","Population.Density","Housing.Density","Sunlight","GDP","Poverty","Unemployed","Children.Poverty","Income.Inequality","Social","PM2.5","Water","SHP","HO","HIF")
df$Water=as.factor(df$Water)

df$Poverty=as.numeric(df$Poverty)
# str(df)

loadPkg("DMwR")
df2=centralImputation(df) #Fill in missing values
# sum(is.na(df2)) #Now there are no missing values

xkablesummary(df2[,c(1,3,4,5,6,7,8,9,10,11,12,18,22)])
```

From the average of the output results, we can see that the average proportion of teenagers under the age of 18 is 22.1%, and the average proportion of people over 65 is 19.3%. The largest number of all races is Non-Hispanic White, with an average proportion of 76.2. The average proportion of women is 49.9, the average proportion of the poor is 15.9%, and the average of the Social Association Rate is 11.6.
We divide the data into four levels according to total cases.

### Which race is the majority of the sample?
```{r}
racemean = data.frame(Race.ratio= c(8.8,2.4,1.5,0.1,9.6,76.2), legend= c('Black','American Indian & Alaska Native','Asian','Native Hawaiian/Other Pacific Islander','Hispanic','Non-Hispanic White'))
p = ggplot(racemean, aes(x = "", y =Race.ratio, fill =legend)) + 
  geom_bar(stat = "identity", width = 1) +  
  coord_polar(theta = "y")   
p
```
According to the average value, we get a pie chart of race proportions, from which we can see the overall proportions of different races. In the following, we will study the proportion of which race is related to the number of confirmed cases and the number of deaths.

## Stay at home policy in each province
### Is the stay at home policy related to the first case coming time?
### Is the stay at home policy after the first case related to the people inflected number?

## Underlying Health Conditions
###  Are there any common underlying health conditions?

## Impact of Temperature


# Chapter 4: Independent Variables EDA: Boxplots, Scatterplots, ANOVA, & Chi-Square

```{r}
#sum(is.na(df2$TC))

break1=fivenum(df2$TC)
break1
labels = c("0-2", "2-9", "9-39", "39-110465")
rank=cut(df2$TC,break1,labels,ordered_result = T)

shapiro.test(df2$TC)
bartlett.test(TC ~ rank, data=df2)
```
The Shapiro-Wilk test is used to test whether the data conforms to the normal distribution.
H0: The sample data is not significantly different from the normal distribution
H1: The sample data is significantly different from the normal distribution
The p-value is less than 0.05, the null hypothesis is rejected, and the total cases do not conform to the normal distribution.

Test for homogeneity of variance
H0: Data with the same variance at different levels
H1: Data without the same variance at different levels
The result shows that the p value is less than 0.05, rejecting the null hypothesis, and total cases do not meet the homogeneity of variance.

## SMART Question: The proportion of which race is related to the number of confirmed cases and deaths
```{r}
race=subset(df2[,c(1,2,6,7,8,9,10,11)])
loadPkg("corrplot")
cor(race)
corrplot.mixed(cor(race))
```
First, let's look at the relationship between the total number of cases and race. It can be seen from the correlation coefficient graph that the positive correlation coefficient between total cases and Asians is the largest, which indicates that the higher the proportion of Asians, the more total cases, and Asian infections may be more serious. The other two races with higher correlation coefficients with TC are Hispanics and blacks. The correlation coefficient between total cases and non-Hispanic whites is negative, indicating that the higher the proportion of this race, the fewer the number of people diagnosed with COVID-19.

Second, we can see the relationship between the number of deaths and race, and the results are consistent with the above results.

## SMART Question: Are the total cases related to age?
```{r boxplot age&gender, echo=FALSE}
loadPkg("plotly")
ggplotly(ggplotly(ggplot(df2, aes(x=rank, y=old, fill=rank)) + geom_boxplot() + scale_fill_brewer(palette="Spectral") + ggtitle("Proportion of elderly over 65 vs. Total cases") + ylab("Proportion of elderly over 65") + xlab("Total cases") +  theme(plot.title= element_text(hjust=0.5, size = 14))))
```

It can be seen from the image that the proportion of elderly people is lower in areas with more total cases.

```{r}
loadPkg('lmtest')
test1 <- bptest(df2$old~rank)
test1$p.value

old.anova <- aov(df2$old~rank)
summary(old.anova)
TKcond <- TukeyHSD(old.anova)
TKcond

par(las=1)
plot(TukeyHSD(old.anova))
```
According to the BP test, the p value is greater than 0.05, and the null hypothesis is accepted: the variance of old is the same under different ranks. The next step ANOVA test can be performed.

According to the ANOVA test, the p value is less than 0.05, and the null hypothesis is rejected: the mean value of old is the same under different ranks. It shows that the proportion of elderly people over 65 years old is different in regions with different confirmed cases.

According to the Tukey HSD test for multiple comparisons, all p values are less than 0.05, so the difference in old between different ranks is statistically significant.

## SMART Question: Are the total cases related to gender?
```{r}
ggplotly(ggplotly(ggplot(df2, aes(x=rank, y=Female, fill=rank)) + geom_boxplot() + scale_fill_brewer(palette="Spectral") + ggtitle("Female vs. Total cases") + ylab("Proportion of female") + xlab("Total cases") +  theme(plot.title= element_text(hjust=0.5, size = 14))))
```

In areas with more total cases, the average percentage of women is higher.

## SMART Question: Are the total cases related to Poverty?
```{r}
ggplotly(ggplotly(ggplot(df2, aes(x=rank, y=Poverty, fill=rank)) + geom_boxplot() + scale_fill_brewer(palette="Spectral") + ggtitle("Poverty vs. Total cases") + ylab("Proportion of Poverty") + xlab("Total cases") +  theme(plot.title= element_text(hjust=0.5, size = 14))))
```

From the first to the third level, the more total cases, the larger the average proportion of poor people. The average proportion of poor people in severely affected areas is the lowest.
## SMART Question:
## SMART Question:
## SMART Question:

# Chapter 5: Linear Regression Model
## SMART Question: What factors influence the death the most?


# Chapter 6: LASSO & Ridge Regression

We convert the two variables Presence of Drinking Water Violation and Stay At Home Order After First Case into categorical variables. Then began to fit LASSO regression and ridge regression. We normalize the data, then split the data into training and test set, so that we can estimate test errors. The split will be used here for Lasso and later for Ridge regression. For brevity, we selected 34 variables for the following analysis.

## LASSO Regression

We draw the plot for different $\lambda$ values to see the overall trend.
```{r}
getwd()
lassodf=data.frame(read.csv("V1.csv", header = TRUE))[,-2]

colnames(lassodf) <- c("TC","population","young","old","black","AIAN","Asian","NH","Hispanic","NHW","Female","Rural","Population.Density","Housing.Density","Sunlight","GDP","Poverty","Unemployed","Children.Poverty","Income.Inequality","Social","PM2.5","Water","SHP","poorhealth","Unhealthy.Days","smokers","Obesity","Physically.ina","WAEO","CRD","Temp","Order")


lassodf$Poverty=as.numeric(lassodf$Poverty)
lassodf$Water=as.factor(lassodf$Water)
lassodf$Order=as.factor(lassodf$Order)
#str(lassodf)

# subset(lassodf, TC != "NA")
lassodf=na.omit(lassodf)

```

```{r uzscale_fcn}
uzscale <- function(df, append=0, excl=NULL) { 
  #' Standardize dataframe to z scores, safe for non-numeric variables. 
  #' ELo 201904 GWU DATS
  #' @param df The dataframe.
  #' @param append T/F or 0/1. Option to append scaled columns or replace original columns in the dataframe.
  #' @param excl A list c(a,b,"d","ef") of excluded columns, either by their indexes and/or names.
  #' @return The transformed dataframe, appended or replaced with standardized scores. Non-numeric columns will not be appended, or if "replace option" is chosen, the columns will be untouched.
  #' @examples
  #' library("ISLR")
  #' tmp = uzscale( Hitters )
  #' tmp = uzscale( Hitters, 1 )
  #' tmp = uzscale( Hitters, TRUE, c(19,"NewLeague") )
  
  append = ifelse(append==TRUE || append=="true" || append=="True" || append=="T" || append=="t" || append==1 || append=="1", TRUE, FALSE) # standardize append 
  nmax = length(df)
  if (nmax < 1 || !is.numeric(nmax) ) { return(df) }
  df1 = df
  onames = colnames(df)  # the original column names
  cnames = onames  # the new column names, if needed start with the original ones
  znames = paste("z",cnames, sep="")     # new column names added prefix 'z'. Those are non-numeric will not be used.
  nadd = ifelse(append, nmax, 0) # add to the column index or replace the orig columns
  j=1  # counting index
  for( i in 1:nmax ) {
    if ( is.numeric(df[,i]) && !( i %in% excl || onames[i] %in% excl ) ) { 
      df1[,j+nadd] = scale(df[,i])
      cnames = c(cnames, znames[i])
      j=j+1
    } else if ( !append ) { j=j+1
    } # if append == 1 and (colunm non-numeric or excluded), do not advance j.
  }
  if (append) { colnames(df1) <- cnames }
  return(df1)
}
```

```{r sd&split}
stdf=uzscale(lassodf, 0)

x=model.matrix(TC~.,stdf)[,-1]
y=stdf$TC

loadPkg("glmnet")
grid=10^seq(5,-5,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments

loadPkg("dplyr")
set.seed(1)
train = stdf %>% sample_frac(0.5)
test = stdf %>% setdiff(train)

x_train = model.matrix(TC~., train)[,-1]
x_test = model.matrix(TC~., test)[,-1]

y_train = train %>% select(TC) %>% unlist() # %>% as.numeric()
y_test = test %>% select(TC) %>% unlist() # %>% as.numeric()
```

```{r lassomodel}
lasso.mod=glmnet(x_train,y_train,alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out.lasso=cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out.lasso)
```

```{r}
bestlam.lasso=cv.out.lasso$lambda.min
cat("lowest lamda from CV: ", bestlam.lasso, "\n\n")
```

We see that the lowest MSE is when $\lambda$ appro = `r round(bestlam.lasso,digits=6)`. 

```{r}
lasso.pred=predict(lasso.mod,s=bestlam.lasso,newx=x_test)
#
out.lasso = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lassoMeanMse = mean((lasso.pred-y_test)^2)
cat("Mean MSE for best Lasso lamda: ", lassoMeanMse, "\n\n")
#
lasso_coef = predict(out.lasso, type = "coefficients", s = bestlam.lasso)[1:13,] # Display coefficients using λ chosen by CV
cat("\nAll the coefficients : \n")
lasso_coef
cat("\nThe non-zero coefficients : \n")
lasso_coef[lasso_coef!=0]
```
From LASSO regression, the coefficients of 11 variables are not zero, the coefficients of the remaining variables become zero. From the results, we can see that race, gender, age, population, population density and rural proportions will all have an impact on total cases.

```{r lasso R2, echo=FALSE}
sst1 <- sum((y_test - mean(y_test))^2)
sse1 <- sum((lasso.pred - y_test)^2)
rsq1 <- 1 - sse1 / sst1
```
We then calculate the R squared of lasso regression, which is `r rsq1`.  

## Ridge Regression
```{r}
grid2=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid2)
dim(coef(ridge.mod))
plot(ridge.mod) 
```
```{r}
#ridge.mod$lambda[50] # 11498
coef(ridge.mod)[,50]
#sqrt(sum(coef(ridge.mod)[-1,50]^2)) # 0.000138
#ridge.mod$lambda[60] # 705
coef(ridge.mod)[,60] 
#sqrt(sum(coef(ridge.mod)[-1,60]^2))  # 0.00224

```
Because the ridge regression uses the "L2 norm", the coefficients are expected to be smaller when $\lambda$ is large. Our "mid-point" (the 50-th) of $\lambda$ equals to 11498, and the sum of squares of coefficients = `r round(sqrt(sum(coef(ridge.mod)[-1,50]^2)),digits=10)`. Compared to the 60-th value (we have a decreasing sequence) $\lambda$ of = 705, we find the sum of squares of the coefficients to be `r round(sqrt(sum(coef(ridge.mod)[-1,60]^2)),digits=7)`, about 16 times larger.

We can use the predict function for various purposes, such as getting the predicted coefficients for $\lambda$=50, for example.

```{r predridge, echo=FALSE}
predict(ridge.mod,s=50,type="coefficients")[1:33,]
```

Then we use the separated training set and test set to see the test error.
```{r predrid, echo=FALSE}
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x_test)
```
The test set mean squared error (MSE) is `r mean((ridge.pred-y_test)^2)`. (We are using standardized scores for $\lambda = 4$.)

```{r mseridge, include=FALSE}
mean((mean(y_train)-y_test)^2) # the test set MSE
```
On the other hand, for the null model ($\lambda$ approaches infinity), the MSE can be found to be `r mean((mean(y_train)-y_test)^2)`. So $\lambda = 4$ reduces the variance by about half, at the expense of bias.

```{r mseridge2, echo=FALSE}
ridge.pred=predict(ridge.mod,s=1e10,newx=x_test)
```
We could have also used a large $\lambda$ value to find the MSE for the null model. These two methods yield essentially the same answer of `r mean((ridge.pred-y_test)^2)`.

```{r mseridg3, echo=FALSE}
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
#mean((ridge.pred - y_test)^2)
predict(ridge.mod, s = 0, type="coefficients")[1:33,]
```

Now for the other extreme special case of small $\lambda$, which is the ordinary least square (OLS) model. We can first use the ridge regression result to predict the $\lambda$ =0 case. The MSE was found to be `r mean((ridge.pred - y_test)^2)` using this result. 

We can also build the OLS model directly, caculate MSE.
```{r, echo=FALSE}
ols.mod = lm(TC~., data = train)
summary(ols.mod)
```
The MSE for OLS regression is `r mean(residuals(ols.mod)^2)`


# Chapter 7: Conclusion
# Chapter 8: Bibliography
Cases in the U.S. (2020, August 01). Retrieved August 01, 2020, from     https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/cases-in-us.html 

